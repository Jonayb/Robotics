<!DOCTYPE html>
<html>
  <head>
    <title>Instructions.md</title>
    <meta http-equiv="Content-type" content="text/html; charset=UTF-8">
    <style>
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: "Segoe WPC", "Segoe UI", "SFUIText-Light", "HelveticaNeue-Light", sans-serif, "Droid Sans Fallback";
	font-size: 14px;
	padding: 0 12px;
	line-height: 22px;
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}


body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	color: #4080D0;
	text-decoration: none;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

h1 code,
h2 code,
h3 code,
h4 code,
h5 code,
h6 code {
	font-size: inherit;
	line-height: auto;
}

a:hover {
	color: #4080D0;
	text-decoration: underline;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left: 5px solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 14px;
	line-height: 19px;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

.mac code {
	font-size: 12px;
	line-height: 18px;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

/** Theming */

.vscode-light,
.vscode-light pre code {
	color: rgb(30, 30, 30);
}

.vscode-dark,
.vscode-dark pre code {
	color: #DDD;
}

.vscode-high-contrast,
.vscode-high-contrast pre code {
	color: white;
}

.vscode-light code {
	color: #A31515;
}

.vscode-dark code {
	color: #D7BA7D;
}

.vscode-light pre:not(.hljs),
.vscode-light code > div {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre:not(.hljs),
.vscode-dark code > div {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre:not(.hljs),
.vscode-high-contrast code > div {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

.vscode-light blockquote,
.vscode-dark blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.vscode-high-contrast blockquote {
	background: transparent;
	border-color: #fff;
}
</style>
    <style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>
    <style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family:  "Meiryo", "Segoe WPC", "Segoe UI", "SFUIText-Light", "HelveticaNeue-Light", sans-serif, "Droid Sans Fallback";
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>
  </head>
  <body>
    <p>LIACS Robotics 2023</p>
    <h1 id="reinforcement-learning-workshop">Reinforcement Learning
      Workshop</h1>
    <p><img src="src/assets/robotics.gif" alt="drawing" height="300"> <img
        src="src/assets/sc2.gif" alt="drawing" height="300"></p>
    <p>This workshop reviews several basics of deep reinforcement
      learning when training agents in some environments in OpenAI gym.</p>
    <ul>
      <li>Before you move on, you should setup the virtual environment
        and install the required packages which could take a while.</li>
    </ul>
    <h2 id="mac-linux">For Mac or Linux</h2>
    <p>Open the terminal and go to this RL-workshop directory (Note: use
      Python 3.8.10 - 3.8.16) :</p>
    <pre class="hljs"><code><div># first install swig: <br>sudo apt install swig<br><br># Skip the following steps if you do already have python3.8.* available! <br># Otherwise, you should install for example python3.8.16<br>sudo apt update &amp;&amp; sudo apt upgrade<br>sudo add-apt-repository ppa:deadsnakes/ppa<br>sudo apt install python3.8.16<br>sudo apt install python3.8-distutils<br>sudo apt install python3.8-dev<br><br># If you do not have the code already, get the workshop code and after unpacking go to the directory RL_Workshop<br>wget 'https://liacs.leidenuniv.nl/~bakkerem2/robotics/RL_Workshop.zip'<br>unzip RL_Workshop.zip<br><br># create a virtual environment 'env'<br>virtualenv env --python=python3.8
source ./env/bin/activate<br><br># Always upgrade pip!<br>pip install --upgrade pip <br><br># Install the necessary packages.<br>chmod u+x install.sh<br>./install.sh<br><br># Do a 'pip list' to check the packages installed in your virtual environment.<br># If Box2D is not in the list, you should build and install the wheel for Box2D-2.3.10:<br>pip install https://github.com/pybox2d/pybox2d/archive/refs/tags/2.3.10.tar.gz<br><br># start the workshop
python src/RLWorkshop.py
</div></code></pre>
    <h2 id="windows">For Windows</h2>
    <p>Run Windows PowerShell as Administrator and execute:</p>
    <pre class="hljs"><code><div>Set-Executionpolicy Unrestricted
</div></code></pre>
    <p>Select the answer [Y] and cd to RL-workshop directory</p>
    <p>Note it is assumed that the following programs are installed:<br>
      - use Python3.8.10<br>
      - Swig4.0.2: download swigwin4.0.2 and add the path to the
      directory with swig.exe to the env-path of the PowerShell using </p>
    <pre class="lang-sh s-code-block hljs bash"><code><span class="hljs-variable">$env</span>:Path += <span class="hljs-string">";PathtoSwigExe"</span></code></pre>
    <p>Then setup the virtual environment and install the necessary
      packages:<br>
    </p>
    <pre class="hljs"><code><div><pre class="hljs"><code># If you do not have the code already, get the workshop code:<br>wget 'https://liacs.leidenuniv.nl/~bakkerem2/robotics/RL_Workshop.zip'<br>unzip RL_Workshop.zip</code></pre>python -m venv env
env\Scripts\Activate.ps1<br>python -m pip install --upgrade pip<br><pre class="hljs"><code>pip install https://github.com/pybox2d/pybox2d/archive/refs/tags/2.3.10.tar.gz</code></pre># Install required packages (this can take a while)<br>.\install.bat

python src/RLWorkshop.py
</div></code></pre>
    <h1 id="reinforcement-learning-theory-basics">Reinforcement learning
      theory basics</h1>
    <p>Reinforcement learning is a framework for learning sequences of
      optimal actions. The main goal is to maximize the cumulative
      reward that the agent receives over multiple timesteps.​</p>
    <p><img src="src/assets/base.png" alt="drawing" height="300"> <img
        src="src/assets/mdp.png" alt="drawing" height="300"></p>
    <p>Reinforcement learning can be understood using the concepts of
      agents, environments, states, actions and rewards, all of which
      will be explained below. Capital letters tend to denote sets of
      things, and lower-case letters denote a specific instance of that
      thing; e.g. A is all possible actions, while a is a specific
      action contained in the set.</p>
    <ol>
      <li>Agent: An agent takes actions in an environment. The RL
        algorithm itself can also be called the agent.</li>
      <li>Action (A): A is the set of all possible moves the agent can
        make. The set of actions can be either discrete or continuous,
        e.g. discrete - [turn left, turn right]; continuous - [turn left
        by 2.0232 degrees, turn right by -0.023 degrees]. Most robotics
        and real world reinforcement learning formulations are
        continuous.</li>
      <li>Discount factor γ: The discount factor is multiplied by future
        rewards that acts as a parameter for controlling how the agent
        prioritizes short-term versus long-term rewards.</li>
      <li>Environment: The world through which the agent moves. The
        environment takes the agent’s current state and action as input,
        and returns as output the agent’s reward and its next state. If
        you are the agent, the environment could be the laws of physics
        (real world) or the rules of the simulation. The agent is also
        considered as part of the environment.</li>
      <li>State (S): A state is a concrete current configuration of the
        environment that the agent is in. Usually it is represented by a
        vector of a specific length that includes the relevant
        descriptors that the agent can use to make decisions.</li>
      <li>Reward (R): A reward is the feedback by which we measure the
        success or failure of an agent’s actions. Rewards can be
        immediate or delayed. They effectively evaluate the agent’s
        action and are represented by a single scalar value.</li>
      <li>Policy (π): The policy is the strategy that the agent employs
        to determine the next action based on the current state. It maps
        states to actions, and its goal is to find the optimal set of
        actions that maximizes the discounted reward.</li>
      <li>Value (V): The expected long-term return with discount, as
        opposed to the short-term reward R. Vπ(s) is defined as the
        expected long-term return of the current state under policy π.
        We discount rewards, or lower their estimated value, the further
        into the future they occur.</li>
      <li>Episode - a set of <em>[state1-action1-state2-action2...state_n,action_n]</em>
        transitions until the agent exceeds the time limit / achieves
        the goal or fails in some critical way.</li>
    </ol>
    <p>So environments are functions that transform an action taken in
      the current state into the next state and a reward; agents are
      functions that transform the new state and reward into the next
      action. We can know the agent’s function, but we cannot know the
      function of the environment. It is a black box where we only see
      the inputs and outputs. Reinforcement learning represents an
      agent’s attempt to approximate the environment’s function, such
      that we can send actions into the black-box environment that
      maximize the rewards it gives out.</p>
    <h1 id="deep-reinforcement-learning">Deep reinforcement learning</h1>
    <img style="float: right;" src="src/assets/ac.png" alt="drawing"
      hspace="20" height="350, "> Most robotics control tasks have
    continuous state and action spaces, therefore the Markov Decision
    Processes that define them are essentially infinite. Since there is
    no way to sample this infinite* space of state-action transitions,
    we need some form of approximation function to get reasonable
    performance and currently most modern methods use deep neural
    networks to achieve this.
    <p>The reinforcement learning algorithm that you are going to be
      using today is Proximal Policy Optimization (PPO) which is one of
      the best performing RL algorithms to date. It is widely used in
      various robotics control tasks and it had many successes when
      applied to complicated environments:</p>
    <ol>
      <li><a href="https://openai.com/blog/openai-five/">OpenAI Five -
          Dota 2</a></li>
      <li><a href="https://openai.com/blog/openai-baselines-ppo/">Various






          simulated robot control tasks</a></li>
    </ol>
    <p>This algorithm uses two neural networks:</p>
    <ol>
      <li>Actor (policy network) - takes the environment state as input
        and produces appropriate actions as outputs. (Look at the policy
        (π) definition 7. above)</li>
      <li>Critic (value network) - takes states and actions as inputs
        and outputs a single scalar value - the estimated cumulative
        discounted reward that the agent is going to aqcuire onwards as
        it makes further actions with the current policy. This output is
        then used as a part of the loss function for both networks.
        (Look at the value (V) definition 8. above).</li>
    </ol>
    <!-- <img src="src/assets/base.png" alt="drawing" height="300"/> -->
    <p>When trained together these networks can solve a wide variety of
      tasks and are perfectly suited for continuous action and state
      spaces. The policies produced by this algorithm are stochastic, as
      instead of learning a specific action, given a state, the agent
      learns the parameters of a distribution of actions from which the
      actions are sampled. Therefore the actions that your agent
      produces are most likely going to be different each time you
      retrain the agent, even when using a constant random seed for
      network weight initialization.</p>
    <h1 id="interface">Interface</h1>
    <img style="float: right;" src="src/assets/interface.png"
      alt="drawing" hspace="20" height="400">
    <p>For your convenience you are provided with an interface that
      makes it easy to control the internal tensorflow training code and
      set up the neural networks and the reinforcement learning
      parameters to solve the problems. To run it:</p>
    <pre class="hljs"><code><div>python3 src/RLWorkshop.py
</div></code></pre>
    <p>Interface guidelines:</p>
    <ol>
      <li>Create environment - initializes the agent in an environment
        selected at the drop down list at the top with the neural
        network architecture as configured in the 'Network' table.</li>
      <li>Train - the agent runs the environment on an episode basis
        (until it exceeds the time limit / achieves the goal or fails in
        some critical way). While training you are shown only the last
        frame of the episode and the neural networks are updated every <em>n</em>
        episodes as indicated by the parameter <em>batch_size</em>. You
        can see plots for average reward per batch and the loss of the
        policy network on the left.</li>
      <li>Test - runs the current policy of an agent in the environment
        step-by-step. The bottom left plot shows the output of the Actor
        (policy network). You can pause the training at any time and use
        this mode to check what your agent is doing exactly during the
        episode in between the updates.</li>
      <li>Reset - destroys the agent and lets you rerun it with a
        different architecture or create a different environment. (Note,
        always do a Reset before starting a new task.)<br>
      </li>
      <li>Record - when the 'Test' mode is on you can start record the
        policies of your agents by pressing 'Record'. Press the same
        button again and the gif of the recording will be saved in the
        current directory.</li>
    </ol>
    <p>Apart from the neural net architectures the other parameters of
      the environments can be changed during run-time, therefore you can
      experiment to achieve better performance (or worse). Each
      parameter has a tool tip that explains its use and the general
      guidelines of how they should be configured depending on the
      complexity of the problem.</p>
    <h1 id="tasks">Tasks</h1>
    <h2 id="1-solving-the-mountaincarcontinuous-v0-environment">1.
      Solving the MountainCarContinuous-v0 environment.</h2>
    <img src="src/assets/mountain.png" alt="drawing" height="300">
    <p>This OpenAI gym environment is a great illustration of a simple
      reinforcement learning problem where the agent has to take
      detrimental actions that give negative rewards in the short term
      in order to get a big reward for completing the task (a primitive
      form of planning). It has a very simple state and action space:
      one action [-1:1] indicating the velocity to left or right, and a
      state consisting of a vector: [position, velocity]. As the agent
      moves towards the goal (flagpole) it receives a positive reward,
      as it moves away from it it receives a negative reward. The agent
      does not have enough torque to just go uphill straight away.</p>
    <p><strong>Task: Try to find good learning parameters and neural
        network architectures that will solve the environment
        (consistently reaching the flagpole) with a reward around 90.
        Note: Given the right parameters the environment can be solved
        in 1-2 network updates.</strong></p>
    <p>Hints:</p>
    <ol>
      <li>The problem is very simple, therefore the neural networks
        required should be small (a couple of hiden layers with ~10
        units each).</li>
      <li>Read the tool tips of the parameters to guide you.</li>
      <li>If the output of the agent is in the range [-1:1] what is the
        required activation function for the actor network? (Look up the
        functions online if you are not sure.)</li>
    </ol>
    <h2 id="2-solving-the-LunarLanderContinuous-v2-environment">2.
      Solving the LunarLanderContinuous-v2 environment.</h2>
    <img src="src/assets/result.gif" alt="drawing" height="300">
    <p>This OpenAI gym environment shows a slightly more complicated
      agent. The landing pad is always at coordinates (0,0). The
      coordinates are the first two numbers in the state vector. The
      reward for moving from the top of the screen to landing at the
      landing pad at zero speed is about 100..140 points. If the lander
      moves away from the landing pad it loses part of the reward.
      Episodes finish, if the lander crashes or comes to rest, receiving
      additional -100 or +100 points. Each leg ground contact is +10
      points. Firing the main engine is -0.3 points for each frame.
      Solving the problem is 200 points. Landing outside the landing pad
      is possible. Fuel is infinite, so an agent can learn to fly and
      then land on its first attempt. Action is two real values vector
      from [-1:1]. The first value controls the main engine, [-1:0]
      engine is off, [0:1] throttle from 50% to 100% power. Engine can't
      work with less than 50% power. The second value is used to fire
      the left or right engine: [-1:-0.5] fire left engine, [0.5:1.0]
      fire right engine, [-0.5~0.5] engine is off.</p>
    <p><strong>Task: Try to propose good learning parameters for which
        you expect to achieve a good reward on average.</strong></p>
    <p>Hints:</p>
    <ol>
      <li>When the action and state spaces grow, you need to increase
        the sizes of hidden layers.</li>
      <li>Learning rates should decrease as complexity increases.</li>
      <li>Note: Try to first define a strategy for finding the optimal
        learning parameters for this problem. <br>
      </li>
      <li><b>Do not spend more than 1 hour trying various proposed
          parameters and report your best results.</b><br>
      </li>
    </ol>
    <h1 id="submission">Submission</h1>
    <p>Submit a small pdf-report (max 1 page including images) of your
      findings and submit it to Bright space. The focus here is on
      explaining what you did and why, what the whole idea/strategy of
      your approach is and how your assessment of that is. Try to give
      some experimental evidence (reward screenshot or gif of the agent)
      for what you conclude. </p>
    <h1 id="questions">Questions</h1>
    <p>If you have any problems running the environments, spot bugs in
      the code or if you have questions regarding reinforcement learning
      workshop in general, don't hesitate to contact us. There are
      several time-slots available on machines with the installed
      workshop. Contact: erwin@liacs.nl <br>
    </p>
  </body>
</html>
